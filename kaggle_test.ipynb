{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU detected: Tesla P100-PCIE-16GB\n",
      "Total GPU Memory: 15.89 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the GPU name\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    # Get GPU properties, including total memory (in bytes)\n",
    "    gpu_properties = torch.cuda.get_device_properties(0)\n",
    "    total_memory_gb = gpu_properties.total_memory / (1024 ** 3)\n",
    "    \n",
    "    print(\"GPU detected:\", gpu_name)\n",
    "    print(f\"Total GPU Memory: {total_memory_gb:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Running on CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, DynamicCache, Cache\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Callable\n",
    "# import flash_attn\n",
    "\n",
    "def _sample_top_p(logits, top_p=0.9):\n",
    "    # First normalize the logits to prevent overflow/underflow\n",
    "    logits = logits - torch.max(logits)\n",
    "    \n",
    "    # Convert to probabilities with softmax\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Apply a small epsilon to avoid numerical issues\n",
    "    eps = 1e-10\n",
    "    probs = torch.clamp(probs, min=eps)\n",
    "    probs = probs / probs.sum()  # Re-normalize\n",
    "    \n",
    "    # Sort the probabilities\n",
    "    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "    \n",
    "    # Compute cumulative probabilities\n",
    "    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "    \n",
    "    # Remove tokens with cumulative probability above the threshold\n",
    "    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "    # Keep the first token above threshold\n",
    "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "    sorted_indices_to_remove[..., 0] = 0\n",
    "    \n",
    "    # Scatter back to original indices\n",
    "    indices_to_remove = sorted_indices_to_remove.scatter(dim=-1, index=sorted_indices, src=sorted_indices_to_remove)\n",
    "    probs = probs.masked_fill(indices_to_remove, 0.0)\n",
    "    \n",
    "    # Re-normalize after masking\n",
    "    probs = probs / (probs.sum() + eps)\n",
    "    \n",
    "    # Check for invalid values before sampling\n",
    "    if torch.isnan(probs).any() or torch.isinf(probs).any() or (probs < 0).any():\n",
    "        # Fix invalid values\n",
    "        probs = torch.nan_to_num(probs, nan=eps, posinf=1.0, neginf=eps)\n",
    "        probs = torch.clamp(probs, min=eps)\n",
    "        probs = probs / probs.sum()  # Re-normalize\n",
    "    \n",
    "    # Sample from the filtered distribution\n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "    \n",
    "    return next_token\n",
    "\n",
    "class WIMInference:\n",
    "\n",
    "    def __init__(\n",
    "        self, model, tokenizer\n",
    "    ) -> None:\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.wim_kv_cache = DynamicCache()\n",
    "        self.classifier_kv_cache = DynamicCache()\n",
    "\n",
    "    def _prefill_tokens(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        cache_positions: torch.Tensor,\n",
    "        kv_cache: Cache,\n",
    "    ):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                cache_position=cache_positions,\n",
    "                use_cache=True,\n",
    "                past_key_values=kv_cache,\n",
    "            )\n",
    "        return outputs\n",
    "\n",
    "    def shrink_kv_cache_from_end(self, new_size: int, kv_cache: Cache):\n",
    "\n",
    "        def resize_tensor_list(token_list):\n",
    "            for layer_idx in range(len(token_list)):\n",
    "                token_list[layer_idx] = token_list[layer_idx][:, :, :new_size, :]\n",
    "\n",
    "        resize_tensor_list(kv_cache.key_cache)\n",
    "        resize_tensor_list(kv_cache.value_cache)\n",
    "        kv_cache._seen_tokens = new_size\n",
    "\n",
    "    def generate_text_with_kv_cache(\n",
    "        self,\n",
    "        max_new_tokens: int,\n",
    "        previous_logits: torch.Tensor,\n",
    "        do_sample: bool,\n",
    "        top_p: float,\n",
    "        temperature: float,\n",
    "        early_stopping: bool,    \n",
    "        kv_cache: Cache,\n",
    "    ) -> str:\n",
    "        generated_tokens = []\n",
    "\n",
    "        # This is needed to create the cache_position tensor\n",
    "        next_token_pos = kv_cache.get_seq_length()\n",
    "\n",
    "        # Use the logits from the prefilling to generate the first token\n",
    "        logits = previous_logits\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Select the last token from the logits\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            if do_sample:\n",
    "                # Divide the logits by the temperature\n",
    "                next_token_logits = next_token_logits / temperature\n",
    "                # Apply the softmax\n",
    "                next_token_probs = torch.nn.functional.softmax(\n",
    "                    next_token_logits, dim=-1\n",
    "                )\n",
    "                \n",
    "                # Check for invalid values (moved here after next_token_probs is defined)\n",
    "                if torch.isnan(next_token_probs).any() or torch.isinf(next_token_probs).any():\n",
    "                    print(\"Invalid probabilities detected!\")\n",
    "                    print(f\"Min prob: {next_token_probs.min()}, Max prob: {next_token_probs.max()}\")\n",
    "                    print(f\"Contains NaN: {torch.isnan(next_token_probs).any()}\")\n",
    "                    print(f\"Contains Inf: {torch.isinf(next_token_probs).any()}\")\n",
    "                    \n",
    "                next_token = _sample_top_p(next_token_logits, top_p)  # Note: passing logits, not probs\n",
    "            else:\n",
    "                # Select the token with the highest probability\n",
    "                next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "            assert next_token.size() == (1, 1)\n",
    "            # Remove the batch dimension\n",
    "            next_token = next_token.squeeze(0)\n",
    "            generated_tokens.append(next_token)\n",
    "            # Stop if we reached the EOS token\n",
    "            if next_token.item() == self.tokenizer.eos_token_id and early_stopping:\n",
    "                break\n",
    "            # Use the generated token as input for the next step\n",
    "            generation_input_ids = next_token.unsqueeze(-1)\n",
    "            kv_cache_seq_len = kv_cache.get_seq_length()\n",
    "            generation_attention_mask = torch.ones(\n",
    "                (1, kv_cache_seq_len + 1), device=next_token.device, dtype=torch.long\n",
    "            )\n",
    "            generation_cache_position = torch.tensor(\n",
    "                [next_token_pos], device=next_token.device\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Get the model outputs\n",
    "                outputs = self.model(\n",
    "                    input_ids=generation_input_ids,\n",
    "                    attention_mask=generation_attention_mask,\n",
    "                    cache_position=generation_cache_position,\n",
    "                    use_cache=True,\n",
    "                    past_key_values=kv_cache,\n",
    "                )\n",
    "            logits = outputs.logits\n",
    "            next_token_pos += 1\n",
    "\n",
    "        generated_tokens = torch.cat(generated_tokens, dim=-1)\n",
    "        # Decode the generated tokens\n",
    "        decoded = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "        return decoded\n",
    "\n",
    "    def prefill_text_with_kv_cache(self, text: str, kv_cache: Cache):\n",
    "        # Tokenize the text\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\")\n",
    "        input_ids = inputs[\"input_ids\"].to(self.model.device)\n",
    "        seq_len = input_ids.size(1)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(self.model.device)\n",
    "\n",
    "        # If we have a KV-Cache, we need to extend the attention mask to account for tokens already in the KV-Cache\n",
    "        if kv_cache.get_seq_length() > 0:\n",
    "            kv_cache_seq_len = kv_cache.get_seq_length()\n",
    "            attention_mask = torch.cat(\n",
    "                [\n",
    "                    torch.ones(\n",
    "                        attention_mask.shape[0],\n",
    "                        kv_cache_seq_len,\n",
    "                        dtype=attention_mask.dtype,\n",
    "                        device=attention_mask.device,\n",
    "                    ),\n",
    "                    attention_mask,\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "        # Generate the cache positions for the tokens to be prefilled\n",
    "        cache_positions = torch.arange(\n",
    "            kv_cache.get_seq_length(), kv_cache.get_seq_length() + seq_len\n",
    "        ).to(self.model.device)\n",
    "        outputs = self._prefill_tokens(input_ids, attention_mask, cache_positions, kv_cache)\n",
    "        return kv_cache.get_seq_length(), seq_len, outputs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "@dataclass\n",
    "class RLConfig:\n",
    "    \"\"\"Configuration for the RL-based margin generation.\"\"\"\n",
    "    learning_rate: float = 5e-6\n",
    "    kl_coef: float = 0.1\n",
    "    discount_factor: float = 0.99\n",
    "    ppo_epochs: int = 4\n",
    "    ppo_mini_batch_size: int = 4\n",
    "    max_grad_norm: float = 0.5\n",
    "    clip_param: float = 0.2\n",
    "    value_loss_coef: float = 0.5\n",
    "    entropy_coef: float = 0.01\n",
    "\n",
    "class MarginRewardModel:\n",
    "    \"\"\"Model to compute rewards for generated margins.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_id: str, device: str = \"cuda\"):\n",
    "        \"\"\"Initialize the reward model.\n",
    "        \n",
    "        Args:\n",
    "            model_id: The ID of the model to use for reward computation.\n",
    "            device: The device to use for computation.\n",
    "        \"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=device,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "        ).eval()\n",
    "        self.device = device\n",
    "        \n",
    "    def compute_reward(self, margins: List[str], query: str, classification_results: List[bool]) -> torch.Tensor:\n",
    "        \"\"\"Compute rewards for a batch of margins.\n",
    "        \n",
    "        The reward combines:\n",
    "        1. Relevance to the query\n",
    "        2. Conciseness (penalize overly verbose margins)\n",
    "        3. Information density\n",
    "        4. Agreement with classifier (higher reward if classifier agrees)\n",
    "        \n",
    "        Args:\n",
    "            margins: List of generated margins\n",
    "            query: The original query\n",
    "            classification_results: Whether each margin was classified as relevant\n",
    "            \n",
    "        Returns:\n",
    "            Tensor of rewards for each margin\n",
    "        \"\"\"\n",
    "        rewards = []\n",
    "        \n",
    "        for margin, is_relevant in zip(margins, classification_results):\n",
    "            with torch.no_grad():\n",
    "                # Construct prompt to evaluate margin quality\n",
    "                prompt = f\"Query: {query}\\nMargin: {margin}\\n\\nRate the quality of this margin from 0 to 10 based on relevance and information density. A good margin should contain key information relevant to the query.\"\n",
    "                \n",
    "                inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs, \n",
    "                    max_new_tokens=5,\n",
    "                    return_dict_in_generate=True,\n",
    "                    output_scores=True,\n",
    "                )\n",
    "                \n",
    "                # Extract score from the generated text\n",
    "                generated_text = self.tokenizer.decode(outputs.sequences[0][-5:], skip_special_tokens=True)\n",
    "                try:\n",
    "                    # Try to extract a numeric score from the response\n",
    "                    score = float(''.join(c for c in generated_text if c.isdigit() or c == '.'))\n",
    "                    # Normalize score to 0-1 range\n",
    "                    score = min(max(score / 10.0, 0.0), 1.0)\n",
    "                except:\n",
    "                    # Default score if parsing fails\n",
    "                    score = 0.5\n",
    "                \n",
    "            # Additional reward components\n",
    "            length_penalty = min(1.0, 100 / max(10, len(margin.split())))  # Prefer concise margins\n",
    "            classifier_agreement = 1.0 if is_relevant else 0.2  # Reward if classifier agrees it's relevant\n",
    "            \n",
    "            # Combine reward components\n",
    "            reward = (0.6 * score) + (0.2 * length_penalty) + (0.2 * classifier_agreement)\n",
    "            rewards.append(reward)\n",
    "            \n",
    "        return torch.tensor(rewards, device=self.device)\n",
    "\n",
    "class RLMarginGenerator:\n",
    "    \"\"\"Generate margins using reinforcement learning.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        model_id: str,\n",
    "        reward_model_id: str = None,\n",
    "        rl_config: RLConfig = None,\n",
    "        device: str = \"cuda\"\n",
    "    ):\n",
    "        \"\"\"Initialize the RL-based margin generator.\n",
    "        \n",
    "        Args:\n",
    "            model_id: The ID of the model to use for generation.\n",
    "            reward_model_id: The ID of the model to use for reward computation.\n",
    "                If None, uses the same model as the generator.\n",
    "            rl_config: Configuration for the RL algorithm.\n",
    "            device: The device to use for computation.\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        \n",
    "        # Initialize the policy model (for generating margins)\n",
    "        print('policy model', model_id)\n",
    "        self.policy_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=device,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "        )\n",
    "        \n",
    "        print('ref model', model_id)\n",
    "        # Initialize the reference model (for KL divergence computation)\n",
    "        self.ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=device,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "        )\n",
    "        for param in self.ref_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in self.policy_model.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "        # Initialize the reward model\n",
    "        if reward_model_id is None:\n",
    "            reward_model_id = model_id\n",
    "        self.reward_model = MarginRewardModel(reward_model_id, device)\n",
    "        \n",
    "        # Initialize the RL config\n",
    "        self.rl_config = rl_config if rl_config is not None else RLConfig()\n",
    "        \n",
    "        # Initialize the optimizer\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.policy_model.parameters(),\n",
    "            lr=self.rl_config.learning_rate\n",
    "        )\n",
    "\n",
    "        if torch.isnan(torch.nn.utils.clip_grad_norm_(\n",
    "            self.policy_model.parameters(),\n",
    "            self.rl_config.max_grad_norm\n",
    "        )):\n",
    "            print(\"Warning: Nan gradiesnts detected\")\n",
    "        \n",
    "        # Initialize the WIM inference\n",
    "        self.wim = WIMInference(self.policy_model, self.tokenizer)\n",
    "        \n",
    "    def _compute_logprobs(self, model, input_ids, attention_mask, labels):\n",
    "        \"\"\"Compute log probabilities for a batch of sequences.\"\"\"\n",
    "        with torch.set_grad_enabled(model.training):\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "            )\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "            \n",
    "            # Get log probs for each token in the sequence\n",
    "            token_log_probs = torch.gather(\n",
    "                log_probs[:, :-1, :], 2, labels[:, 1:, None]\n",
    "            ).squeeze(-1)\n",
    "            \n",
    "            # Mask out padding tokens\n",
    "            mask = (labels[:, 1:] != self.tokenizer.pad_token_id).float()\n",
    "            token_log_probs = token_log_probs * mask\n",
    "            \n",
    "            # Sum log probs over sequence\n",
    "            seq_log_probs = token_log_probs.sum(dim=1)\n",
    "            \n",
    "            return seq_log_probs\n",
    "            \n",
    "    def _compute_kl_divergence(self, input_ids, attention_mask, labels):\n",
    "        \"\"\"Compute KL divergence between policy and reference model with better numerical stability.\"\"\"\n",
    "        # Small constant to avoid division by zero or log(0)\n",
    "        epsilon = 1e-8\n",
    "        \n",
    "        # Get logits from both models\n",
    "        with torch.set_grad_enabled(self.policy_model.training):\n",
    "            policy_outputs = self.policy_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            policy_logits = policy_outputs.logits\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            ref_outputs = self.ref_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            ref_logits = ref_outputs.logits\n",
    "        \n",
    "        # Focus only on the predicted token positions\n",
    "        seq_length = policy_logits.size(1)\n",
    "        vocab_size = policy_logits.size(2)\n",
    "        \n",
    "        # Make sure labels are properly shaped\n",
    "        if labels.dim() == 2:\n",
    "            labels = labels.unsqueeze(-1)\n",
    "        \n",
    "        # Get the masked positions (not -100)\n",
    "        valid_positions = (labels[:, 1:, 0] != -100)\n",
    "        \n",
    "        # Extract logits for valid positions\n",
    "        policy_logits = policy_logits[:, :-1].reshape(-1, vocab_size)\n",
    "        ref_logits = ref_logits[:, :-1].reshape(-1, vocab_size)\n",
    "        \n",
    "        # Convert to probabilities\n",
    "        policy_probs = torch.nn.functional.softmax(policy_logits, dim=-1)\n",
    "        ref_probs = torch.nn.functional.softmax(ref_logits, dim=-1)\n",
    "        \n",
    "        # Apply clipping to avoid numerical issues\n",
    "        policy_probs = torch.clamp(policy_probs, min=epsilon, max=1.0-epsilon)\n",
    "        ref_probs = torch.clamp(ref_probs, min=epsilon, max=1.0-epsilon)\n",
    "        \n",
    "        # Calculate KL divergence: p * log(p/q)\n",
    "        kl_div = policy_probs * (torch.log(policy_probs) - torch.log(ref_probs))\n",
    "        \n",
    "        # Sum over vocabulary dimension\n",
    "        kl_div = kl_div.sum(dim=-1)\n",
    "        \n",
    "        # Reshape back to batch_size x seq_length\n",
    "        kl_div = kl_div.reshape(-1, seq_length - 1)\n",
    "        \n",
    "        # Apply masking for valid positions\n",
    "        if valid_positions.any():\n",
    "            kl_div = kl_div * valid_positions.float()\n",
    "            # Average over valid positions\n",
    "            kl_div = kl_div.sum(dim=1) / (valid_positions.sum(dim=1).float() + epsilon)\n",
    "        else:\n",
    "            kl_div = kl_div.mean(dim=1)\n",
    "        \n",
    "        return kl_div\n",
    "    \n",
    "    def generate_rl_margin(\n",
    "        self,\n",
    "        segment: str,\n",
    "        query: str,\n",
    "        extractive_summary_prompt: str,\n",
    "        classification_prompt: str,\n",
    "        max_new_tokens: int = 100,\n",
    "        do_sample: bool = True,\n",
    "        top_p: float = 0.9,\n",
    "        temperature: float = 1.0,\n",
    "        early_stopping: bool = True,\n",
    "    ):\n",
    "        \"\"\"Generate a margin using the current policy model.\"\"\"\n",
    "        # Prefill the segment\n",
    "        prefilled_tokens_before_extractive_summary, _, _ = self.wim.prefill_text_with_kv_cache(\n",
    "            segment, self.wim.wim_kv_cache\n",
    "        )\n",
    "        \n",
    "        # Prefill the extractive summary prompt\n",
    "        _, _, extractive_summary_outputs = self.wim.prefill_text_with_kv_cache(\n",
    "            extractive_summary_prompt.format(query=query), self.wim.wim_kv_cache\n",
    "        )\n",
    "        \n",
    "        # Generate the margin\n",
    "        margin = self.wim.generate_text_with_kv_cache(\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            previous_logits=extractive_summary_outputs[\"logits\"],\n",
    "            do_sample=do_sample,\n",
    "            top_p=top_p,\n",
    "            temperature=temperature,\n",
    "            early_stopping=early_stopping,\n",
    "            kv_cache=self.wim.wim_kv_cache,\n",
    "        )\n",
    "        \n",
    "        # Shrink the KV cache back to before the extractive summary prompt\n",
    "        self.wim.shrink_kv_cache_from_end(\n",
    "            new_size=prefilled_tokens_before_extractive_summary,\n",
    "            kv_cache=self.wim.wim_kv_cache,\n",
    "        )\n",
    "        \n",
    "        # Classify the margin\n",
    "        classification_input = classification_prompt.format(query=query, answer=margin)\n",
    "        _, _, classification_prompt_logits = self.wim.prefill_text_with_kv_cache(\n",
    "            classification_input, self.wim.classifier_kv_cache\n",
    "        )\n",
    "        \n",
    "        classification_output = self.wim.generate_text_with_kv_cache(\n",
    "            max_new_tokens=10,\n",
    "            previous_logits=classification_prompt_logits[\"logits\"],\n",
    "            do_sample=False,\n",
    "            top_p=0.9,\n",
    "            temperature=1.0,\n",
    "            early_stopping=early_stopping,\n",
    "            kv_cache=self.wim.classifier_kv_cache,\n",
    "        )\n",
    "        \n",
    "        # Parse the classification output\n",
    "        is_relevant = self._parse_classifier_output(classification_output)\n",
    "        \n",
    "        # Clear the classifier KV cache\n",
    "        self.wim.shrink_kv_cache_from_end(\n",
    "            new_size=0, kv_cache=self.wim.classifier_kv_cache\n",
    "        )\n",
    "        \n",
    "        return margin, is_relevant\n",
    "        \n",
    "    def _parse_classifier_output(self, output: str) -> bool:\n",
    "        \"\"\"Parse the classification output to determine if the margin is relevant.\"\"\"\n",
    "        output = output.replace(\"```\", \"\").strip()\n",
    "        output = output.split(\"#\")[0]\n",
    "        if output.endswith(\"YES\"):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "            \n",
    "    def train_rl_margin_generator(\n",
    "        self,\n",
    "        segments: List[str],\n",
    "        query: str,\n",
    "        extractive_summary_prompt: str,\n",
    "        classification_prompt: str,\n",
    "        num_episodes: int = 10,\n",
    "        max_new_tokens: int = 100,\n",
    "    ):\n",
    "        \"\"\"Train the margin generator using PPO.\"\"\"\n",
    "        \n",
    "        # Set model to training mode explicitly\n",
    "        self.policy_model.train()\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            print(f\"Episode {episode + 1}/{num_episodes}\")\n",
    "            \n",
    "            # Sample a batch of segments\n",
    "            batch_size = min(self.rl_config.ppo_mini_batch_size, len(segments))\n",
    "            segment_indices = np.random.choice(len(segments), batch_size, replace=False)\n",
    "            batch_segments = [segments[i] for i in segment_indices]\n",
    "            \n",
    "            # Generate margins using the current policy\n",
    "            margins = []\n",
    "            is_relevant_list = []\n",
    "            \n",
    "            for segment in tqdm(batch_segments, desc=\"Generating margins\"):\n",
    "                # Clear KV caches\n",
    "                self.wim.shrink_kv_cache_from_end(0, self.wim.wim_kv_cache)\n",
    "                self.wim.shrink_kv_cache_from_end(0, self.wim.classifier_kv_cache)\n",
    "                \n",
    "                margin, is_relevant = self.generate_rl_margin(\n",
    "                    segment=segment,\n",
    "                    query=query,\n",
    "                    extractive_summary_prompt=extractive_summary_prompt,\n",
    "                    classification_prompt=classification_prompt,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                )\n",
    "                \n",
    "                margins.append(margin)\n",
    "                is_relevant_list.append(is_relevant)\n",
    "            \n",
    "            # Compute rewards for the generated margins\n",
    "            with torch.no_grad():\n",
    "                rewards = self.reward_model.compute_reward(margins, query, is_relevant_list)\n",
    "            \n",
    "            # Prepare inputs for PPO update\n",
    "            inputs = []\n",
    "            for segment, margin in zip(batch_segments, margins):\n",
    "                # Tokenize the segment + extractive summary prompt + margin\n",
    "                context = segment + extractive_summary_prompt.format(query=query) + margin\n",
    "                input_tokens = self.tokenizer(context, return_tensors=\"pt\", padding=True).to(self.device)\n",
    "                \n",
    "                # Create labels for computing log probs\n",
    "                labels = input_tokens.input_ids.clone()\n",
    "                # Mask out tokens we don't want to compute loss for\n",
    "                context_without_margin = segment + extractive_summary_prompt.format(query=query)\n",
    "                context_tokens = len(self.tokenizer(context_without_margin, return_tensors=\"pt\").input_ids[0])\n",
    "                labels[:, :context_tokens] = -100  # Mask out non-margin tokens\n",
    "                \n",
    "                inputs.append({\n",
    "                    \"input_ids\": input_tokens.input_ids,\n",
    "                    \"attention_mask\": input_tokens.attention_mask,\n",
    "                    \"labels\": labels,\n",
    "                })\n",
    "            \n",
    "            # PPO update\n",
    "            for _ in range(self.rl_config.ppo_epochs):\n",
    "                for i in range(len(inputs)):\n",
    "                    self.optimizer.zero_grad()\n",
    "                    \n",
    "                    # Get model outputs with gradients enabled\n",
    "                    outputs = self.policy_model(\n",
    "                        input_ids=inputs[i][\"input_ids\"],\n",
    "                        attention_mask=inputs[i][\"attention_mask\"],\n",
    "                        labels=inputs[i][\"labels\"]\n",
    "                    )\n",
    "                    \n",
    "                    # Model loss already has gradients\n",
    "                    model_loss = outputs.loss\n",
    "                    \n",
    "                    # Compute KL divergence\n",
    "                    kl_div = self._compute_kl_divergence(\n",
    "                        inputs[i][\"input_ids\"],\n",
    "                        inputs[i][\"attention_mask\"],\n",
    "                        inputs[i][\"labels\"],\n",
    "                    )\n",
    "                    \n",
    "                    # Compute policy loss - make sure reward is detached\n",
    "                    reward_term = rewards[i].detach() * -1.0  # Negative since we want to maximize reward\n",
    "                    kl_term = self.rl_config.kl_coef * kl_div.detach()\n",
    "                    \n",
    "                    # Combine with model loss to ensure gradients flow\n",
    "                    policy_loss = model_loss + reward_term + kl_term\n",
    "                    \n",
    "                    # Backward pass\n",
    "                    policy_loss.backward()\n",
    "                    \n",
    "                    # Clip gradients\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        self.policy_model.parameters(),\n",
    "                        self.rl_config.max_grad_norm,\n",
    "                    )\n",
    "                    \n",
    "                    # Update policy model\n",
    "                    self.optimizer.step()\n",
    "            \n",
    "            # Log metrics\n",
    "            avg_reward = rewards.mean().item()\n",
    "            relevance_rate = sum(is_relevant_list) / len(is_relevant_list)\n",
    "            \n",
    "            print(f\"Average reward: {avg_reward:.4f}\")\n",
    "            print(f\"Average KL divergence: {kl_div.mean().item():.4f}\")\n",
    "            print(f\"Relevance rate: {relevance_rate:.4f}\")\n",
    "    \n",
    "    def save_model(self, output_dir: str):\n",
    "        \"\"\"Save the trained model.\"\"\"\n",
    "        self.policy_model.save_pretrained(output_dir)\n",
    "        self.tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from wim import WIMInference\n",
    "# from RL_margin_generation import RLMarginGenerator\n",
    "\n",
    "import tiktoken\n",
    "from nltk import sent_tokenize\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import os\n",
    "\n",
    "class WIMRLInference(WIMInference):\n",
    "    \"\"\"Extended WIM inference class that uses RL-trained model for margin generation.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, rl_margin_generator=None):\n",
    "        \"\"\"Initialize the WIM inference with an optional RL margin generator.\n",
    "        \n",
    "        Args:\n",
    "            model: The base model for generation.\n",
    "            tokenizer: The tokenizer.\n",
    "            rl_margin_generator: Optional RL-based margin generator.\n",
    "        \"\"\"\n",
    "        super().__init__(model, tokenizer)\n",
    "        self.rl_margin_generator = rl_margin_generator\n",
    "        \n",
    "    def process_with_rl_margins(\n",
    "        self,\n",
    "        context: str,\n",
    "        query: str,\n",
    "        system_message: str,\n",
    "        extractive_summary_prompt: str,\n",
    "        classification_prompt: str,\n",
    "        final_answer_prompt: str,\n",
    "        min_tokens_segment: int = 4096,\n",
    "        max_new_tokens_extractive_summary: int = 100,\n",
    "        max_new_tokens_final_answer: int = 100,\n",
    "        max_new_tokens_classification: int = 10,\n",
    "        do_sample: bool = True,\n",
    "        top_p: float = 0.9,\n",
    "        temperature: float = 1.0,\n",
    "        early_stopping: bool = True,\n",
    "        use_rl_generator: bool = True,\n",
    "        print_step_summary: bool = False,\n",
    "    ):\n",
    "        \"\"\"Process the context using the WIM approach with RL-enhanced margin generation.\n",
    "        \n",
    "        Args:\n",
    "            context: The full document context.\n",
    "            query: The user query.\n",
    "            system_message: The system message prompt.\n",
    "            extractive_summary_prompt: Template for extractive summary prompt.\n",
    "            classification_prompt: Template for classification prompt.\n",
    "            final_answer_prompt: Template for final answer prompt.\n",
    "            min_tokens_segment: Minimum number of tokens per segment.\n",
    "            max_new_tokens_extractive_summary: Maximum number of tokens to generate for margin.\n",
    "            max_new_tokens_final_answer: Maximum number of tokens to generate for final answer.\n",
    "            max_new_tokens_classification: Maximum number of tokens to generate for classification.\n",
    "            do_sample: Whether to use sampling for generation.\n",
    "            top_p: Top-p sampling parameter.\n",
    "            temperature: Temperature for generation.\n",
    "            early_stopping: Whether to use early stopping.\n",
    "            use_rl_generator: Whether to use the RL-based margin generator.\n",
    "            print_step_summary: Whether to print a summary for each step.\n",
    "            \n",
    "        Returns:\n",
    "            final_answer: The generated answer.\n",
    "            positive_margins: List of relevant margins used.\n",
    "        \"\"\"\n",
    "        # Clear KV caches\n",
    "        self.shrink_kv_cache_from_end(0, self.wim_kv_cache)\n",
    "        self.shrink_kv_cache_from_end(0, self.classifier_kv_cache)\n",
    "        \n",
    "        # Segment the context\n",
    "        segments = self._chunk_text_to_segments(context, min_tokens_segment)\n",
    "        \n",
    "        # Prefill the system message\n",
    "        _, _, _ = self.prefill_text_with_kv_cache(system_message, self.wim_kv_cache)\n",
    "        \n",
    "        positive_margins = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for segment_index in range(len(segments)):\n",
    "                segment = segments[segment_index]\n",
    "                \n",
    "                if use_rl_generator and self.rl_margin_generator is not None:\n",
    "                    print(\"# Use RL-based margin generator\")\n",
    "                    margin, is_relevant = self.rl_margin_generator.generate_rl_margin(\n",
    "                        segment=segment,\n",
    "                        query=query,\n",
    "                        extractive_summary_prompt=extractive_summary_prompt,\n",
    "                        classification_prompt=classification_prompt,\n",
    "                        max_new_tokens=max_new_tokens_extractive_summary,\n",
    "                        do_sample=do_sample,\n",
    "                        top_p=top_p,\n",
    "                        temperature=temperature,\n",
    "                        early_stopping=early_stopping,\n",
    "                    )\n",
    "                else:\n",
    "                    print('# Use standard WIM approach')\n",
    "                    prefilled_tokens_before_extractive_summary, _, _ = self.prefill_text_with_kv_cache(\n",
    "                        segment, self.wim_kv_cache\n",
    "                    )\n",
    "                    \n",
    "                    formatted_extractive_summary = extractive_summary_prompt.format(query=query)\n",
    "                    _, _, extractive_summary_outputs = self.prefill_text_with_kv_cache(\n",
    "                        formatted_extractive_summary, self.wim_kv_cache\n",
    "                    )\n",
    "                    \n",
    "                    margin = self.generate_text_with_kv_cache(\n",
    "                        max_new_tokens=max_new_tokens_extractive_summary,\n",
    "                        previous_logits=extractive_summary_outputs[\"logits\"],\n",
    "                        do_sample=do_sample,\n",
    "                        top_p=top_p,\n",
    "                        temperature=temperature,\n",
    "                        early_stopping=early_stopping,\n",
    "                        kv_cache=self.wim_kv_cache,\n",
    "                    )\n",
    "                    \n",
    "                    # Shrink KV cache back to before extractive summary\n",
    "                    self.shrink_kv_cache_from_end(\n",
    "                        new_size=prefilled_tokens_before_extractive_summary,\n",
    "                        kv_cache=self.wim_kv_cache,\n",
    "                    )\n",
    "                    \n",
    "                    # Classify the margin\n",
    "                    classification_input = classification_prompt.format(query=query, answer=margin)\n",
    "                    _, _, classification_prompt_logits = self.prefill_text_with_kv_cache(\n",
    "                        classification_input, self.classifier_kv_cache\n",
    "                    )\n",
    "                    \n",
    "                    classification_output = self.generate_text_with_kv_cache(\n",
    "                        max_new_tokens=max_new_tokens_classification,\n",
    "                        previous_logits=classification_prompt_logits[\"logits\"],\n",
    "                        do_sample=False,\n",
    "                        top_p=top_p,\n",
    "                        temperature=temperature,\n",
    "                        early_stopping=early_stopping,\n",
    "                        kv_cache=self.classifier_kv_cache,\n",
    "                    )\n",
    "                    \n",
    "                    is_relevant = self._parse_classifier_output(classification_output)\n",
    "                    \n",
    "                    # Clear the classifier KV cache\n",
    "                    self.shrink_kv_cache_from_end(\n",
    "                        new_size=0, kv_cache=self.classifier_kv_cache\n",
    "                    )\n",
    "                \n",
    "                if is_relevant:\n",
    "                    positive_margins.append(margin)\n",
    "                \n",
    "                if print_step_summary:\n",
    "                    print({\n",
    "                        \"step\": segment_index,\n",
    "                        \"prefilled_tokens_so_far\": self.wim_kv_cache.get_seq_length(),\n",
    "                        \"margin\": margin.strip(),\n",
    "                        \"classification_result\": is_relevant,\n",
    "                    })\n",
    "            \n",
    "            # Prefill the concatenated margins and the prompt to ask the final answer\n",
    "            concatenated_margins = \"\".join(positive_margins)\n",
    "            formatted_final_answer = final_answer_prompt.format(\n",
    "                margins=concatenated_margins, query=query\n",
    "            )\n",
    "            \n",
    "            _, _, final_answer_prefill_outputs = self.prefill_text_with_kv_cache(\n",
    "                formatted_final_answer, self.wim_kv_cache\n",
    "            )\n",
    "            \n",
    "            # Generate the final answer\n",
    "            final_answer = self.generate_text_with_kv_cache(\n",
    "                max_new_tokens=max_new_tokens_final_answer,\n",
    "                previous_logits=final_answer_prefill_outputs[\"logits\"],\n",
    "                do_sample=do_sample,\n",
    "                top_p=top_p,\n",
    "                temperature=temperature,\n",
    "                early_stopping=early_stopping,\n",
    "                kv_cache=self.wim_kv_cache,\n",
    "            )\n",
    "            \n",
    "            return final_answer, positive_margins\n",
    "    \n",
    "    def _chunk_text_to_segments(self, text, min_tokens_segment=4096):\n",
    "        \"\"\"Chunk text into segments of approximately min_tokens_segment tokens.\"\"\"\n",
    "        \n",
    "        \n",
    "        tokenizer = tiktoken.encoding_for_model(\"gpt-4-turbo\")\n",
    "        segments = []\n",
    "        current_segment = \"\"\n",
    "        sentences = sent_tokenize(text)\n",
    "        curr_tokens = 0\n",
    "        \n",
    "        for line in sentences:\n",
    "            tokens = len(tokenizer.encode(line))\n",
    "            if curr_tokens + tokens > min_tokens_segment:\n",
    "                segments.append(current_segment)\n",
    "                current_segment = \"\"\n",
    "                curr_tokens = 0\n",
    "            \n",
    "            current_segment += line + \" \"\n",
    "            curr_tokens += tokens\n",
    "        \n",
    "        if current_segment:\n",
    "            segments.append(current_segment)\n",
    "        \n",
    "        return segments\n",
    "    \n",
    "    def _parse_classifier_output(self, output: str) -> bool:\n",
    "        \"\"\"Parse the classification output to determine if the margin is relevant.\"\"\"\n",
    "        output = output.replace(\"```\", \"\").strip()\n",
    "        output = output.split(\"#\")[0]\n",
    "        if output.endswith(\"YES\"):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "def run_wim_rl(\n",
    "    model_id: str,\n",
    "    model_id_rl: str,\n",
    "    input_document: str,\n",
    "    query: str,\n",
    "    use_rl_generator: bool = True,\n",
    "    train_rl_generator: bool = False,\n",
    "    num_episodes: int = 10,\n",
    "    output_model_dir: str = None,\n",
    "    # attn_implementation: str = \"flash_attention_2\",\n",
    "    attn_implementation:str = 'sdpa',\n",
    "    dtype: str = \"bfloat16\",\n",
    "    min_tokens_segment: int = 4096,\n",
    "    max_new_tokens_extractive_summary: int = 100,\n",
    "    max_new_tokens_final_answer: int = 50,\n",
    "    max_new_tokens_classification: int = 10,\n",
    "    do_sample: bool = True,\n",
    "    top_p: float = 0.9,\n",
    "    temperature: float = 1.0,\n",
    "    early_stopping: bool = True,\n",
    "    print_step_summary: bool = False,\n",
    "    user_header: str = \"\",\n",
    "    generation_header: str = \"\",\n",
    "):\n",
    "    \"\"\"Run WIM with RL-enhanced margin generation.\n",
    "    \n",
    "    Args:\n",
    "        model_id: The ID of the model to use.\n",
    "        input_document: The input document content.\n",
    "        query: The user query.\n",
    "        use_rl_generator: Whether to use the RL-based margin generator.\n",
    "        train_rl_generator: Whether to train the RL generator.\n",
    "        num_episodes: Number of episodes for RL training.\n",
    "        output_model_dir: Directory to save the trained model.\n",
    "        attn_implementation: Attention implementation to use.\n",
    "        dtype: Data type for model weights.\n",
    "        min_tokens_segment: Minimum number of tokens per segment.\n",
    "        max_new_tokens_extractive_summary: Maximum number of tokens to generate for margin.\n",
    "        max_new_tokens_final_answer: Maximum number of tokens to generate for final answer.\n",
    "        max_new_tokens_classification: Maximum number of tokens to generate for classification.\n",
    "        do_sample: Whether to use sampling for generation.\n",
    "        top_p: Top-p sampling parameter.\n",
    "        temperature: Temperature for generation.\n",
    "        early_stopping: Whether to use early stopping.\n",
    "        print_step_summary: Whether to print a summary for each step.\n",
    "        user_header: User header for the model.\n",
    "        generation_header: Generation header for the model.\n",
    "        \n",
    "    Returns:\n",
    "        final_answer: The generated answer.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Define model dtype\n",
    "    model_dtype = torch.float32\n",
    "    if dtype == \"float16\":\n",
    "        model_dtype = torch.float16\n",
    "    elif dtype == \"float32\":\n",
    "        model_dtype = torch.float32\n",
    "    elif dtype == \"bfloat16\":\n",
    "        model_dtype = torch.bfloat16\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer_rl = AutoTokenizer.from_pretrained(model_id_rl)\n",
    "    tokenizer_rl.pad_token = tokenizer_rl.eos_token\n",
    "    # quant_config = BitsAndBytesConfig(\n",
    "    #     load_in_4bit=True,\n",
    "    #     bnb_4bit_compute_dtype=torch.float16,  # could also try bfloat16\n",
    "    #     bnb_4bit_use_double_quant=True,\n",
    "    #     bnb_4bit_quant_type=\"nf4\"  # best performance for LLaMA-like models\n",
    "    # )\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        attn_implementation=attn_implementation,\n",
    "        torch_dtype=model_dtype,\n",
    "    ).eval()\n",
    "\n",
    "    model_rl = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id_rl,\n",
    "        device_map=\"auto\",\n",
    "        attn_implementation=attn_implementation,\n",
    "        torch_dtype=model_dtype,\n",
    "    ).eval()\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Load templates\n",
    "    template_extractive_summary = \"\"\"\n",
    "    ```\n",
    "    Given the above context, extract all information relevant to the query: \"{query}\". If the context is not relevant to the query, answer \"I don't know.\"\n",
    "    {generation_header}\n",
    "    \"\"\".strip()\n",
    "    \n",
    "    template_classification = \"\"\"\n",
    "    {user_header}\n",
    "    I asked an LLM assistant whether a piece of document is related to the query: \"{query}\". This is its answer: \n",
    "    ```text\n",
    "    {answer}\n",
    "    ```\n",
    "    Should I save it for later? \n",
    "    Here are rules:\n",
    "    - Answer YES if the answer contains information about the query. \n",
    "    - Answer NO if the answer says the piece isn't related to the query.\n",
    "\n",
    "    Provide the answer in the format: <YES/NO>#<Explanation>. \n",
    "    Here is are example answers:\n",
    "\n",
    "    YES#Yes, the information contains an excerpt from a book that is related to the question.\n",
    "    NO#No, the LLM assistant concluded the information isn't relevant.\n",
    "\n",
    "    Don't add any other comments, all your remarks should be included in the \"Explanation\" section.\n",
    "    {generation_header}\n",
    "    \"\"\".strip()\n",
    "    \n",
    "    template_system_message = \"\"\"\n",
    "    {user_header}\n",
    "    ```\n",
    "    \"\"\".strip()\n",
    "    \n",
    "    template_final_answer = \"\"\"\n",
    "    ```\n",
    "    {margins}\n",
    "    {query}\n",
    "    {generation_header}\n",
    "    \"\"\".strip()\n",
    "    \n",
    "    # Replace special tokens\n",
    "    special_tokens = {\n",
    "        \"{user_header}\": \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "        \"{generation_header}\": \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "    }\n",
    "    \n",
    "    for token, replacement in special_tokens.items():\n",
    "        template_extractive_summary = template_extractive_summary.replace(token, replacement)\n",
    "        template_classification = template_classification.replace(token, replacement)\n",
    "        template_system_message = template_system_message.replace(token, replacement)\n",
    "        template_final_answer = template_final_answer.replace(token, replacement)\n",
    "    \n",
    "    # Create WIM inference\n",
    "    # wim_inference = WIMInference(model, tokenizer)\n",
    "\n",
    "\n",
    "    # Create WIM RL inference for training RL model\n",
    "    wim_inference = WIMRLInference(model, tokenizer)\n",
    "    \n",
    "    # Create RL margin generator if needed\n",
    "    rl_margin_generator = None\n",
    "    if use_rl_generator or train_rl_generator:\n",
    "        print(\"Use RL-based margin generator\")\n",
    "        rl_margin_generator = RLMarginGenerator(\n",
    "            model_id=model_id_rl,\n",
    "            reward_model_id=model_id_rl,  # Use same model for rewards\n",
    "            device=\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available else \"cpu\",\n",
    "        )\n",
    "    else:\n",
    "        print(\"Use standard WIM approach\")\n",
    "    # Train RL generator if requested\n",
    "    if train_rl_generator and rl_margin_generator is not None:\n",
    "        print(\"Training RL margin generator...\")\n",
    "        segments = wim_inference._chunk_text_to_segments(input_document, min_tokens_segment)\n",
    "        rl_margin_generator.train_rl_margin_generator(\n",
    "            segments=segments,\n",
    "            query=query,\n",
    "            extractive_summary_prompt=template_extractive_summary,\n",
    "            classification_prompt=template_classification,\n",
    "            num_episodes=num_episodes,\n",
    "            max_new_tokens=max_new_tokens_extractive_summary,\n",
    "        )\n",
    "        \n",
    "        # Save the trained model if requested\n",
    "        if output_model_dir is not None:\n",
    "            print(f\"Saving trained model to {output_model_dir}...\")\n",
    "            rl_margin_generator.save_model(output_model_dir)\n",
    "    \n",
    "    # Create WIM RL inference\n",
    "    wim_rl_inference = WIMRLInference(model, tokenizer, rl_margin_generator)\n",
    "    \n",
    "    # Process with WIM RL\n",
    "    final_answer, positive_margins = wim_rl_inference.process_with_rl_margins(\n",
    "        context=input_document,\n",
    "        query=query,\n",
    "        system_message=template_system_message,\n",
    "        extractive_summary_prompt=template_extractive_summary,\n",
    "        classification_prompt=template_classification,\n",
    "        final_answer_prompt=template_final_answer,\n",
    "        min_tokens_segment=min_tokens_segment,\n",
    "        max_new_tokens_extractive_summary=max_new_tokens_extractive_summary,\n",
    "        max_new_tokens_final_answer = max_new_tokens_final_answer,\n",
    "        max_new_tokens_classification = max_new_tokens_classification,\n",
    "        do_sample = do_sample,\n",
    "        top_p = top_p,\n",
    "        temperature = temperature,\n",
    "        early_stopping = early_stopping,\n",
    "        use_rl_generator = use_rl_generator,\n",
    "        print_step_summary = True\n",
    "    )\n",
    "\n",
    "    return final_answer, positive_margins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Paste your token here\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aef4bcf78a54e6b9ff997c3dce322e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55541202c8024ce19038c2583ba7bcca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59ffe38b4d664bb6af00555458fcc857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b9c03bbdad7453faa646f6c7f4e1905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3978194f88f749eabe94c6ba397f57b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/914k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c18ee72967ef4d89a09ef16063a3e717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/4.38M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bee02c1b861644ea86cf2e9e41af63c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/547 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef87d0eff6841649febfc0f0e55391a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3ad55fddbf54ff3b62380b5629e5218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b68cfa671d42dea4417f5ba9ab31e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "781911af023b49828316c3383586fcb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/673 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8176168bc14544efbfc91e2c2ae8feb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/488M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f23aa156ff434a20af338c325993d5de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use RL-based margin generator\n",
      "policy model shubvhamgore18218/WiM_llama\n",
      "ref model shubvhamgore18218/WiM_llama\n",
      "Training RL margin generator...\n",
      "Episode 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating margins: 100%|██████████| 1/1 [00:03<00:00,  3.12s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 0.5400\n",
      "Average KL divergence: 0.0007\n",
      "Relevance rate: 0.0000\n",
      "Episode 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating margins: 100%|██████████| 1/1 [00:01<00:00,  1.89s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 0.5400\n",
      "Average KL divergence: 0.0007\n",
      "Relevance rate: 0.0000\n",
      "Episode 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating margins: 100%|██████████| 1/1 [00:01<00:00,  1.96s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 0.5400\n",
      "Average KL divergence: 0.0003\n",
      "Relevance rate: 0.0000\n",
      "Episode 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating margins: 100%|██████████| 1/1 [00:01<00:00,  1.96s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 0.5400\n",
      "Average KL divergence: 0.0008\n",
      "Relevance rate: 0.0000\n",
      "Episode 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating margins: 100%|██████████| 1/1 [00:01<00:00,  1.84s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 0.5400\n",
      "Average KL divergence: 0.0031\n",
      "Relevance rate: 0.0000\n",
      "Episode 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating margins: 100%|██████████| 1/1 [00:01<00:00,  1.95s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 0.5400\n",
      "Average KL divergence: 0.0022\n",
      "Relevance rate: 0.0000\n",
      "Episode 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating margins: 100%|██████████| 1/1 [00:01<00:00,  1.86s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 0.5400\n",
      "Average KL divergence: 0.0035\n",
      "Relevance rate: 0.0000\n",
      "Episode 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating margins: 100%|██████████| 1/1 [00:01<00:00,  1.83s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 0.5400\n",
      "Average KL divergence: 0.0027\n",
      "Relevance rate: 0.0000\n",
      "Episode 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating margins: 100%|██████████| 1/1 [00:01<00:00,  1.82s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 0.5400\n",
      "Average KL divergence: 0.0034\n",
      "Relevance rate: 0.0000\n",
      "Episode 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating margins: 100%|██████████| 1/1 [00:01<00:00,  1.88s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 0.5400\n",
      "Average KL divergence: 0.0036\n",
      "Relevance rate: 0.0000\n",
      "# Use RL-based margin generator\n",
      "{'step': 0, 'prefilled_tokens_so_far': 7, 'margin': \"Context: Symbol 1998 (), also called the tishop, is a political new version of three punnels by Mos.  It was based on a dozen single attack reference to this installment was originally played into a wedding back for the rebuilt battle.\\nWarderty Fox {'100, Rion and The Fall of LLC was the fourth problem of a North American lawyer (Eastern) fought in the night of 1996,\", 'classification_result': False}\n",
      "Final Answer:\n",
      "Silky Epeira is a popular beauty influencer and model based in the United States.\n",
      "\n",
      "Positive Margins:\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Define your model and input parameters\n",
    "    # model_id = \"HachiML/TinyLlama2-jp-122M-FlashAttention2\"  # or any compatible model identifier\n",
    "    model_id = \"meta-llama/Llama-3.2-1B-Instruct\"  # or any compatible model identifier\n",
    "    model_id_rl = 'shubvhamgore18218/WiM_llama'\n",
    "    # input_document = \"Your long document text goes here...\"\n",
    "    query = \"Who is silky Epeira?\"\n",
    "    \n",
    "    # Set parameters for margin generation and RL training\n",
    "    final_answer, positive_margins = run_wim_rl(\n",
    "        model_id=model_id,\n",
    "        model_id_rl=model_id_rl,\n",
    "        input_document='/kaggle/input/examples/babilong_8k.json',\n",
    "        query=query,\n",
    "        use_rl_generator=True,      # Use the RL-enhanced margin generator\n",
    "        train_rl_generator=True,   # Set True if you want to train the RL generator\n",
    "        num_episodes=10             # Number of training episodes (only used if training)\n",
    "    )\n",
    "    \n",
    "    print(\"Final Answer:\")\n",
    "    print(final_answer)\n",
    "    print(\"\\nPositive Margins:\")\n",
    "    for margin in positive_margins:\n",
    "        print(margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
